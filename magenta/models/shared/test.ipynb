{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#=============================#\n",
    "||                           ||\n",
    "||                           ||\n",
    "||        PFNN Class         ||\n",
    "||                           ||\n",
    "||                           ||\n",
    "#=============================#\n",
    "\n",
    "Classes for Phase Functioned Neural Networks.\n",
    "We will be implementing these from scratch.\n",
    "\n",
    "We will be basing these off of code in Theano\n",
    "from The Orange Duck found here:\n",
    "https://github.com/sreyafrancis/PFNN\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.layers import base as base_layer\n",
    "from tensorflow.python.ops.rnn_cell_impl import RNNCell\n",
    "from tensorflow.python.ops import random_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from math import pi, floor\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import rnn_cell_impl\n",
    "from tensorflow.contrib.rnn import AttentionCellWrapper\n",
    "from tensorflow.contrib.rnn import DropoutWrapper\n",
    "\n",
    "sigmoid = math_ops.sigmoid\n",
    "tanh = math_ops.tanh\n",
    "\n",
    "class PhaseFunctionedFFNN(base_layer.Layer):\n",
    "\n",
    "    # only one layer for demonstration purposes\n",
    "    def __init__(self, input_shape, output_shape, dropout=0.5):\n",
    "        self.phases = 4\n",
    "        self.x = tf.placeholder(tf.float32, [None, input_shape])\n",
    "        self.W0 = [tf.Variable(tf.zeros([input_shape, output_shape])) for _ in range(self.phases)]\n",
    "        self.b0 = [tf.Variable(tf.zeros([output_shape])) for _ in range(self.phases)]\n",
    "        self.layers = [self.W0, self.b0]\n",
    "\n",
    "        return\n",
    "\n",
    "    def __call__(self, input):\n",
    "        phase = input[-1]\n",
    "        input = input[:-1]\n",
    "        phase_num = (4 * phase) / (2 * pi)\n",
    "\n",
    "        phase_depth = phase_num % 1 # how far into the current phase we are\n",
    "        k = lambda n: (floor(phase_num) + n - 1) % 4\n",
    "        W0_phase = self.cubic_spline(self.W0[k(0)], self.W0[k(1)], self.W0[k(2)], self.W0[k(3)], w)\n",
    "        b0_phase = self.cubi|c_spline(self.b0[k(0)], self.b0[k(1)], self.b0[k(2)], self.b0[k(3)], w)\n",
    "\n",
    "        return tf.matmul(W0_phase, input) + b0_phase\n",
    "\n",
    "    def cubic_spline(self, y0, y1, y2, y3, mu):\n",
    "        return ( \\\n",
    "            (-0.5*y0+1.5*y1-1.5*y2+0.5*y3)*mu*mu*mu + \\\n",
    "            (y0-2.5*y1+2.0*y2-0.5*y3)*mu*mu + \\\n",
    "            (-0.5*y0+0.5*y2)*mu + \\\n",
    "            (y1))\n",
    "\n",
    "\n",
    "\n",
    "PFLSTMStateTuple = collections.namedtuple(\"PFLSTMStateTuple\", (\"c\", \"h\"))\n",
    "\n",
    "class PhaseFunctionedLSTM(RNNCell):\n",
    "\n",
    "    def __init__(self, input_shape, output_shape, dropout=0.5):\n",
    "        self.phases = 4\n",
    "\n",
    "        self.forget_gate = [tf.Variable(tf.zeros([input_shape, output_shape]))] * self.phases\n",
    "        self.forget_bias = [tf.Variable(tf.zeros([output_shape]))] * self.phases\n",
    "\n",
    "        self.input_gate = [tf.Variable(tf.zeros([input_shape, output_shape]))] * self.phases\n",
    "        self.input_bias = [tf.Variable(tf.zeros([output_shape]))] * self.phases\n",
    "\n",
    "        self.new_input = [tf.Variable(tf.zeros([input_shape, output_shape]))] * self.phases\n",
    "        self.new_bias = [tf.Variable(tf.zeros([output_shape]))] * self.phases\n",
    "\n",
    "        self.output_gate = [tf.Variable(tf.zeros([input_shape, output_shape]))] * self.phases\n",
    "        self.output_bias = [tf.Variable(tf.zeros([output_shape]))] * self.phases\n",
    "\n",
    "        self.layers = [self.forget_gate, self.forget_bias, self.input_gate, self.input_bias, \\\n",
    "                        self.new_input, self.new_bias, self.output_gate, self.output_bias]\n",
    "        return\n",
    "\n",
    "    def __call__(self, input, state):\n",
    "                # (c, h) = state\n",
    "                # input = x\n",
    "        # right now assumes only one input at a time (i.e. input is just a vector)\n",
    "        h = state[1]\n",
    "        x = input\n",
    "        phase = input[-1]\n",
    "        input = input[:-1]\n",
    "        phase_num = (4 * phase) / (2 * pi) # assumes phase is from 0 - 2pi\n",
    "\n",
    "        phase_depth = phase_num % 1 # how far into the current phase we are\n",
    "        k = lambda n: (floor(phase_num) + n - 1) % 4 # control point selector function\n",
    "\n",
    "        # indices 0-1 = forget, 2-3 = input, 4-5 = new, 6-7 = output\n",
    "        phased_layers = []\n",
    "        for layer in self.layers:\n",
    "            interpolated = self.cubic_spline(self.layer[k(0)], self.layer[k(1)], self.layer[k(2)], self.layer[k(3)], w)\n",
    "            phased_layers.append(interpolated) # W values\n",
    "\n",
    "            concat = tf.concat([h, x], 1)\n",
    "            W_f = phased_layers[0] # forget Weights\n",
    "            b_f = phased_layers[1] # forget bias\n",
    "            W_i = phased_layers[2] # input Weights\n",
    "            b_i = phased_layers[3] # input bias\n",
    "            W_c = phased_layers[4] # new input weights\n",
    "            b_c = phased_layers[5] # new input bias\n",
    "            W_o = phased_layers[6] # output weights\n",
    "            b_o = phased_layers[7] # output bias\n",
    "            f = sigmoid(tf.matmul(W_f, concat) + b_f)\n",
    "            i = sigmoid(tf.matmul(W_i, concat) + b_i)\n",
    "            C_tilde = tanh(tf.matmul(W_c, concat) + b_c)\n",
    "            o = sigmoid(tf.matmul(W_o, concat + b_o))\n",
    "            new_c = f * c + i * C_tilde\n",
    "            new_h = o * tanh(new_c)\n",
    "            new_state = PFLSTMStateTuple(new_c, new_h)\n",
    "\n",
    "        return (new_h, new_state)\n",
    "\n",
    "\n",
    "    def cubic_spline(self, y0, y1, y2, y3, mu):\n",
    "        return ( \\\n",
    "            (-0.5*y0+1.5*y1-1.5*y2+0.5*y3)*mu*mu*mu + \\\n",
    "            (y0-2.5*y1+2.0*y2-0.5*y3)*mu*mu + \\\n",
    "            (-0.5*y0+0.5*y2)*mu + \\\n",
    "            (y1))\n",
    "\n",
    "\n",
    "class PhaseAttentionCellWrapper(AttentionCellWrapper):\n",
    "    \"\"\"Changing basic attention cell wrapper to incorporate phase.\n",
    "    Implementation based on https://arxiv.org/abs/1409.0473.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cell, attn_length, attn_size=None, attn_vec_size=None,\n",
    "               input_size=None, state_is_tuple=True, reuse=None):\n",
    "        \"\"\"Create a cell with attention.\n",
    "        Args:\n",
    "          cell: an RNNCell, an attention is added to it.\n",
    "          attn_length: integer, the size of an attention window.\n",
    "          attn_size: integer, the size of an attention vector. Equal to\n",
    "              cell.output_size by default.\n",
    "          attn_vec_size: integer, the number of convolutional features calculated\n",
    "              on attention state and a size of the hidden layer built from\n",
    "              base cell state. Equal attn_size to by default.\n",
    "          input_size: integer, the size of a hidden linear layer,\n",
    "              built from inputs and attention. Derived from the input tensor\n",
    "              by default.\n",
    "          state_is_tuple: If True, accepted and returned states are n-tuples, where\n",
    "            `n = len(cells)`.  By default (False), the states are all\n",
    "            concatenated along the column axis.\n",
    "          reuse: (optional) Python boolean describing whether to reuse variables\n",
    "            in an existing scope.  If not `True`, and the existing scope already has\n",
    "            the given variables, an error is raised.\n",
    "        Raises:\n",
    "          TypeError: if cell is not an RNNCell.\n",
    "          ValueError: if cell returns a state tuple but the flag\n",
    "              `state_is_tuple` is `False` or if attn_length is zero or less.\n",
    "        \"\"\"\n",
    "        super(AttentionCellWrapper, self).__init__(_reuse=reuse)\n",
    "        if not rnn_cell_impl._like_rnncell(cell):  # pylint: disable=protected-access\n",
    "            raise TypeError(\"The parameter cell is not RNNCell.\")\n",
    "        if nest.is_sequence(cell.state_size) and not state_is_tuple:\n",
    "            raise ValueError(\"Cell returns tuple of states, but the flag \"\n",
    "                           \"state_is_tuple is not set. State size is: %s\"\n",
    "                           % str(cell.state_size))\n",
    "        if attn_length <= 0:\n",
    "            raise ValueError(\"attn_length should be greater than zero, got %s\"\n",
    "                           % str(attn_length))\n",
    "        if not state_is_tuple:\n",
    "            logging.warn(\n",
    "              \"%s: Using a concatenated state is slower and will soon be \"\n",
    "              \"deprecated.  Use state_is_tuple=True.\", self)\n",
    "        if attn_size is None:\n",
    "            attn_size = cell.output_size\n",
    "        if attn_vec_size is None:\n",
    "            attn_vec_size = attn_size\n",
    "        self._state_is_tuple = state_is_tuple\n",
    "        self._cell = cell\n",
    "        self._attn_vec_size = attn_vec_size\n",
    "        self._input_size = input_size -1 \t# discount phase\n",
    "        self._attn_size = attn_size\n",
    "        self._attn_length = attn_length\n",
    "        self._reuse = reuse\n",
    "        self._linear1 = None\n",
    "        self._linear2 = None\n",
    "        self._linear3 = None\n",
    "        self.phase = None\n",
    "\n",
    "    def call(self, inputs, state):\n",
    "        \"\"\"Long short-term memory cell with attention (LSTMA).\"\"\"\n",
    "\n",
    "        # store phase, shorten inputs\n",
    "        self.phase = inputs[-1]\n",
    "        inputs = inputs[:-1]\n",
    "\n",
    "        if self._state_is_tuple:\n",
    "            state, attns, attn_states = state\n",
    "        else:\n",
    "            states = state\n",
    "            state = array_ops.slice(states, [0, 0], [-1, self._cell.state_size])\n",
    "            attns = array_ops.slice(\n",
    "              states, [0, self._cell.state_size], [-1, self._attn_size])\n",
    "            attn_states = array_ops.slice(\n",
    "              states, [0, self._cell.state_size + self._attn_size],\n",
    "              [-1, self._attn_size * self._attn_length])\n",
    "        attn_states = array_ops.reshape(attn_states,\n",
    "                                        [-1, self._attn_length, self._attn_size])\n",
    "        input_size = self._input_size\n",
    "        if input_size is None:\n",
    "            input_size = inputs.get_shape().as_list()[1]\n",
    "        if self._linear1 is None:\n",
    "            self._linear1 = _Linear([inputs, attns], input_size, True)\n",
    "\n",
    "        inputs = self._linear1([inputs, attns])\n",
    "\n",
    "        # append phase back into input so that PFNN can use it\n",
    "        inputs.append(self.phase)\n",
    "\n",
    "        cell_output, new_state = self._cell(inputs, state)\n",
    "        if self._state_is_tuple:\n",
    "            new_state_cat = array_ops.concat(nest.flatten(new_state), 1)\n",
    "        else:\n",
    "            new_state_cat = new_state\n",
    "        new_attns, new_attn_states = self._attention(new_state_cat, attn_states)\n",
    "        with vs.variable_scope(\"attn_output_projection\"):\n",
    "            if self._linear2 is None:\n",
    "                self._linear2 = _Linear([cell_output, new_attns], self._attn_size, True)\n",
    "            output = self._linear2([cell_output, new_attns])\n",
    "        new_attn_states = array_ops.concat(\n",
    "            [new_attn_states, array_ops.expand_dims(output, 1)], 1)\n",
    "        new_attn_states = array_ops.reshape(\n",
    "            new_attn_states, [-1, self._attn_length * self._attn_size])\n",
    "        new_state = (new_state, new_attns, new_attn_states)\n",
    "        if not self._state_is_tuple:\n",
    "            new_state = array_ops.concat(list(new_state), 1)\n",
    "\n",
    "        return output, new_state\n",
    "\n",
    "class PhaseDropoutWrapper(DropoutWrapper):\n",
    "    \"\"\"Operator adding dropout to inputs and outputs of the given cell.\n",
    "     Incorporates phase. \"\"\"\n",
    "\n",
    "    def __init__(self, cell, input_keep_prob=1.0, output_keep_prob=1.0,\n",
    "        state_keep_prob=1.0, variational_recurrent=False,\n",
    "        input_size=None, dtype=None, seed=None,\n",
    "        dropout_state_filter_visitor=None):\n",
    "\n",
    "        super().__init__(self, cell, input_keep_prob, output_keep_prob, state_keep_prob, variational_recurrent,\n",
    "            input_size, dtype, seed, dropout_state_filter_visitor)\n",
    "\n",
    "        #don't know if I need this\n",
    "        if input_size:\n",
    "            self.input_size = input_size -1\n",
    "\n",
    "        self.phase = None\n",
    "        return\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        \"\"\"Run the cell with the declared dropouts.\"\"\"\n",
    "\n",
    "        # store phase value\n",
    "        self.phase = inputs[-1]\n",
    "        inputs = inputs[:-1]\n",
    "\n",
    "        def _should_dropout(p):\n",
    "             return (not isinstance(p, float)) or p < 1\n",
    "\n",
    "        if _should_dropout(self._input_keep_prob):\n",
    "            inputs = self._dropout(inputs, \"input\",\n",
    "                                 self._recurrent_input_noise,\n",
    "                                 self._input_keep_prob)\n",
    "\n",
    "        # re-append phase so PFNN can use it\n",
    "        inputs.append(self.phase)\n",
    "\n",
    "        output, new_state = self._cell(inputs, state, scope)\n",
    "\n",
    "        if _should_dropout(self._state_keep_prob):\n",
    "        #       Identify which subsets of the state to perform dropout on and\n",
    "          # which ones to keep.\n",
    "            shallow_filtered_substructure = nest.get_traverse_shallow_structure(\n",
    "                                    self._dropout_state_filter, new_state)\n",
    "            new_state = self._dropout(new_state, \"state\",\n",
    "                                    self._recurrent_state_noise,\n",
    "                                    self._state_keep_prob,\n",
    "                                    shallow_filtered_substructure)\n",
    "        if _should_dropout(self._output_keep_prob):\n",
    "            output = self._dropout(output, \"output\",\n",
    "                                 self._recurrent_output_noise,\n",
    "                                 self._output_keep_prob)\n",
    "        return output, new_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (pfnn.py, line 169)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/Users/arshzahed/dev/launchpad/magenta/magenta/models/shared/pfnn.py\"\u001b[0;36m, line \u001b[0;32m169\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m       \n^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2016 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Provides function to build an event sequence RNN model's graph.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from pfnn import PhaseFunctionedLSTM\n",
    "\n",
    "# internal imports\n",
    "import numpy as np\n",
    "import six\n",
    "import tensorflow as tf\n",
    "import magenta\n",
    "\n",
    "from tensorflow.python.util import nest as tf_nest\n",
    "\n",
    "# def make_pfnn_cell(rnn_layer_sizes,\n",
    "#                   dropout_keep_prob=1.0,\n",
    "#                   attn_length=0,\n",
    "#                   base_cell=magenta.models.shared.pfnn):\n",
    "#   return make_rnn_cell(rnn_layer_sizes,\n",
    "#                   dropout_keep_prob,\n",
    "#                   attn_length,\n",
    "#                   base_cell)\n",
    "\n",
    "def make_rnn_cell(rnn_layer_sizes,\n",
    "                  dropout_keep_prob=1.0,\n",
    "                  attn_length=0,\n",
    "                  base_cell=PhaseFunctionedLSTM):\n",
    "  \"\"\"Makes a RNN cell from the given hyperparameters.\n",
    "\n",
    "  Args:\n",
    "    rnn_layer_sizes: A list of integer sizes (in units) for each layer of the\n",
    "        RNN.\n",
    "    dropout_keep_prob: The float probability to keep the output of any given\n",
    "        sub-cell.\n",
    "    attn_length: The size of the attention vector.\n",
    "    base_cell: The base tf.contrib.rnn.RNNCell to use for sub-cells.\n",
    "\n",
    "  Returns:\n",
    "      A tf.contrib.rnn.MultiRNNCell based on the given hyperparameters.\n",
    "  \"\"\"\n",
    "  cells = []\n",
    "  for num_units in rnn_layer_sizes:\n",
    "    cell = base_cell(num_units)\n",
    "    if attn_length and not cells:\n",
    "      # Add attention wrapper to first layer.\n",
    "      cell = tf.contrib.rnn.AttentionCellWrapper(\n",
    "          cell, attn_length, state_is_tuple=True)\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(\n",
    "        cell, output_keep_prob=dropout_keep_prob)\n",
    "    cells.append(cell)\n",
    "\n",
    "  cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "  return cell\n",
    "\n",
    "# make_rnn_cell = make_pfnn_cell\n",
    "\n",
    "\n",
    "def build_graph(mode, config, sequence_example_file_paths=None):\n",
    "    \"\"\"Builds the TensorFlow graph.\n",
    "\n",
    "    Args:\n",
    "    mode: 'train', 'eval', or 'generate'. Only mode related ops are added to\n",
    "        the graph.\n",
    "    config: An EventSequenceRnnConfig containing the encoder/decoder and HParams\n",
    "        to use.\n",
    "    sequence_example_file_paths: A list of paths to TFRecord files containing\n",
    "        tf.train.SequenceExample protos. Only needed for training and\n",
    "        evaluation.\n",
    "\n",
    "    Returns:\n",
    "    A tf.Graph instance which contains the TF ops.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If mode is not 'train', 'eval', or 'generate'.\n",
    "    \"\"\"\n",
    "    if mode not in ('train', 'eval', 'generate'):\n",
    "    raise ValueError(\"The mode parameter must be 'train', 'eval', \"\n",
    "                     \"or 'generate'. The mode parameter was: %s\" % mode)\n",
    "\n",
    "    hparams = config.hparams\n",
    "    encoder_decoder = config.encoder_decoder\n",
    "\n",
    "    tf.logging.info('hparams = %s', hparams.values())\n",
    "\n",
    "    input_size = encoder_decoder.input_size\n",
    "    num_classes = encoder_decoder.num_classes\n",
    "    no_event_label = encoder_decoder.default_event_label\n",
    "\n",
    "    with tf.Graph().as_default() as graph:\n",
    "    inputs, labels, lengths = None, None, None\n",
    "\n",
    "    if mode == 'train' or mode == 'eval':\n",
    "      inputs, labels, lengths = magenta.common.get_padded_batch(\n",
    "          sequence_example_file_paths, hparams.batch_size, input_size,\n",
    "          shuffle=mode == 'train')\n",
    "\n",
    "    elif mode == 'generate':\n",
    "      inputs = tf.placeholder(tf.float32, [hparams.batch_size, None,\n",
    "                                           input_size])\n",
    "\n",
    "    cell = make_rnn_cell(\n",
    "        hparams.rnn_layer_sizes,\n",
    "        dropout_keep_prob=(\n",
    "            1.0 if mode == 'generate' else hparams.dropout_keep_prob),\n",
    "        attn_length=(\n",
    "            hparams.attn_length if hasattr(hparams, 'attn_length') else 0))\n",
    "\n",
    "    initial_state = cell.zero_state(hparams.batch_size, tf.float32)\n",
    "\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(\n",
    "        cell, inputs, sequence_length=lengths, initial_state=initial_state,\n",
    "        swap_memory=True)\n",
    "\n",
    "    outputs_flat = magenta.common.flatten_maybe_padded_sequences(\n",
    "        outputs, lengths)\n",
    "    logits_flat = tf.contrib.layers.linear(outputs_flat, num_classes)\n",
    "\n",
    "    if mode == 'train' or mode == 'eval':\n",
    "      labels_flat = magenta.common.flatten_maybe_padded_sequences(\n",
    "          labels, lengths)\n",
    "\n",
    "      softmax_cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "          labels=labels_flat, logits=logits_flat)\n",
    "\n",
    "      predictions_flat = tf.argmax(logits_flat, axis=1)\n",
    "      correct_predictions = tf.to_float(\n",
    "          tf.equal(labels_flat, predictions_flat))\n",
    "      event_positions = tf.to_float(tf.not_equal(labels_flat, no_event_label))\n",
    "      no_event_positions = tf.to_float(tf.equal(labels_flat, no_event_label))\n",
    "\n",
    "      # Compute the total number of time steps across all sequences in the\n",
    "      # batch. For some models this will be different from the number of RNN\n",
    "      # steps.\n",
    "      def batch_labels_to_num_steps(batch_labels, lengths):\n",
    "        num_steps = 0\n",
    "        for labels, length in zip(batch_labels, lengths):\n",
    "          num_steps += encoder_decoder.labels_to_num_steps(labels[:length])\n",
    "        return np.float32(num_steps)\n",
    "      num_steps = tf.py_func(\n",
    "          batch_labels_to_num_steps, [labels, lengths], tf.float32)\n",
    "\n",
    "        if mode == 'train':\n",
    "        loss = tf.reduce_mean(softmax_cross_entropy)\n",
    "        perplexity = tf.exp(loss)\n",
    "        accuracy = tf.reduce_mean(correct_predictions)\n",
    "        event_accuracy = (\n",
    "            tf.reduce_sum(correct_predictions * event_positions) /\n",
    "            tf.reduce_sum(event_positions))\n",
    "        no_event_accuracy = (\n",
    "            tf.reduce_sum(correct_predictions * no_event_positions) /\n",
    "            tf.reduce_sum(no_event_positions))\n",
    "\n",
    "        loss_per_step = tf.reduce_sum(softmax_cross_entropy) / num_steps\n",
    "        perplexity_per_step = tf.exp(loss_per_step)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=hparams.learning_rate)\n",
    "\n",
    "        train_op = tf.contrib.slim.learning.create_train_op(\n",
    "            loss, optimizer, clip_gradient_norm=hparams.clip_norm)\n",
    "        tf.add_to_collection('train_op', train_op)\n",
    "\n",
    "        vars_to_summarize = {\n",
    "            'loss': loss,\n",
    "            'metrics/perplexity': perplexity,\n",
    "            'metrics/accuracy': accuracy,\n",
    "            'metrics/event_accuracy': event_accuracy,\n",
    "            'metrics/no_event_accuracy': no_event_accuracy,\n",
    "            'metrics/loss_per_step': loss_per_step,\n",
    "            'metrics/perplexity_per_step': perplexity_per_step,\n",
    "        }\n",
    "        elif mode == 'eval':\n",
    "        vars_to_summarize, update_ops = tf.contrib.metrics.aggregate_metric_map(\n",
    "            {\n",
    "                'loss': tf.metrics.mean(softmax_cross_entropy),\n",
    "                'metrics/accuracy': tf.metrics.accuracy(\n",
    "                    labels_flat, predictions_flat),\n",
    "                'metrics/per_class_accuracy':\n",
    "                    tf.metrics.mean_per_class_accuracy(\n",
    "                        labels_flat, predictions_flat, num_classes),\n",
    "                'metrics/event_accuracy': tf.metrics.recall(\n",
    "                    event_positions, correct_predictions),\n",
    "                'metrics/no_event_accuracy': tf.metrics.recall(\n",
    "                    no_event_positions, correct_predictions),\n",
    "                'metrics/loss_per_step': tf.metrics.mean(\n",
    "                    tf.reduce_sum(softmax_cross_entropy) / num_steps,\n",
    "                    weights=num_steps),\n",
    "            })\n",
    "        for updates_op in update_ops.values():\n",
    "            tf.add_to_collection('eval_ops', updates_op)\n",
    "\n",
    "        # Perplexity is just exp(loss) and doesn't need its own update op.\n",
    "        vars_to_summarize['metrics/perplexity'] = tf.exp(\n",
    "            vars_to_summarize['loss'])\n",
    "        vars_to_summarize['metrics/perplexity_per_step'] = tf.exp(\n",
    "            vars_to_summarize['metrics/loss_per_step'])\n",
    "\n",
    "        for var_name, var_value in six.iteritems(vars_to_summarize):\n",
    "            tf.summary.scalar(var_name, var_value)\n",
    "            tf.add_to_collection(var_name, var_value)\n",
    "\n",
    "    elif mode == 'generate':\n",
    "        temperature = tf.placeholder(tf.float32, [])\n",
    "        softmax_flat = tf.nn.softmax(\n",
    "          tf.div(logits_flat, tf.fill([num_classes], temperature)))\n",
    "        softmax = tf.reshape(softmax_flat, [hparams.batch_size, -1, num_classes])\n",
    "\n",
    "        tf.add_to_collection('inputs', inputs)\n",
    "        tf.add_to_collection('temperature', temperature)\n",
    "        tf.add_to_collection('softmax', softmax)\n",
    "        # Flatten state tuples for metagraph compatibility.\n",
    "        for state in tf_nest.flatten(initial_state):\n",
    "        tf.add_to_collection('initial_state', state)\n",
    "        for state in tf_nest.flatten(final_state):\n",
    "        tf.add_to_collection('final_state', state)\n",
    "\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
